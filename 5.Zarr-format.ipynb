{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fdb5508-0c83-4b62-8ccd-a9c580f10e3a",
   "metadata": {},
   "source": [
    "<img src=\"img/era-temp2.png\" alt=\"ERA Temperature\" style=\"width:60%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a32fa-902f-4b75-a326-184b8fd7d0f9",
   "metadata": {},
   "source": [
    "# Exploring ERA5 Weather Data with Zarr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d0608-eb77-4e16-9b61-801c976ebf28",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5533b52-b4cb-4f29-aa87-193baa32ca93",
   "metadata": {},
   "source": [
    "This demo showcases how to efficiently access and analyze large climate datasets using the Zarr format with ERA5 data stored on Google Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e537acc3-69d2-4260-bc3c-b5d66fca438e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## 1. Introduction to ERA5\n",
    "\n",
    "ERA5 is the fifth generation of atmospheric reanalysis data from the European Centre for Medium-Range Weather Forecasts (ECMWF). \n",
    "\n",
    "### What is Reanalysis?\n",
    "Reanalysis is a scientific method that creates a consistent, best-estimate reconstruction of Earth's past weather and climate by combining:\n",
    "\n",
    "Historical Observations:\n",
    "\n",
    "1. Surface stations\n",
    "1. Weather balloons\n",
    "1. Satellites\n",
    "1. Ships and buoys\n",
    "1. Aircraft reports\n",
    "\n",
    "Numerical Weather Models:\n",
    "\n",
    "1. Modern physics equations\n",
    "1. Data assimilation techniques\n",
    "1. Supercomputing power\n",
    "\n",
    "### Key Characteristics:\n",
    "\n",
    "- **\"Best guess\"** of past weather: Fills gaps where direct observations are missing\n",
    "- **Temporally consistent**: Uses one modern system for entire period (unaffected by changing observation technologies)\n",
    "- **Spatially complete**: Provides global coverage even over oceans/polar regions\n",
    "\n",
    "### Key Features:\n",
    "- **Global coverage**: Data for the entire Earth at high resolution\n",
    "- **Temporal range**: 1950 to present (with hourly data from 1979)\n",
    "- **Spatial resolution**: 0.25Â° latitude/longitude (~31 km grid)\n",
    "- **Variables**: 240+ weather parameters (temperature, precipitation, wind, etc.)\n",
    "- **Data assimilation**: Combines models with observations for consistency\n",
    "\n",
    "### Relevant Resources:\n",
    "1. [Official ECMWF ERA5 Documentation](https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5)\n",
    "2. [ERA5 Data Access Guide](https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation)\n",
    "3. [Google Cloud Public Dataset Info](https://cloud.google.com/storage/docs/public-datasets/era5)\n",
    "4. [ARCO-ERA5 Project](https://github.com/google-research/arco-era5)\n",
    "5. [Copernicus Climate Data Store](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ec39a-c8b7-4630-9915-881562d29af7",
   "metadata": {},
   "source": [
    "## 2. Accessing ERA5 Data in Zarr format\n",
    "\n",
    "Now we'll open the [ERA5 dataset](https://cloud.google.com/storage/docs/public-datasets/era5) stored in Zarr format on Google Cloud Storage.\n",
    "\n",
    "Let's check first what is inside the cloud bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16f628-c3c1-44c5-9ab1-6e1c7fe4ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "\n",
    "fs = fsspec.filesystem('gcs', token='anon')\n",
    "store_path = 'gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3'\n",
    "files = fs.ls(store_path)\n",
    "\n",
    "for f in files[:10]:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a194fa7-29c0-488b-a5cb-8420de47e957",
   "metadata": {},
   "source": [
    "Remember how a zarr store is structured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de0cd7-708d-4a10-ae0a-22b94472888d",
   "metadata": {},
   "source": [
    "<img src=\"https://zarr-specs.readthedocs.io/en/latest/_images/terminology-hierarchy.excalidraw.png\" alt=\"Zarr Hierarchy\" style=\"width:40%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138cfa58-4bee-49fd-aa03-dcc56f638e81",
   "metadata": {},
   "source": [
    "Now, we can look inside the `.zmetadata` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c06ac-9c1b-4c2b-9d10-183dbada1e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "fs = fsspec.filesystem('gcs', token='anon')\n",
    "path = 'gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3/.zmetadata'\n",
    "\n",
    "with fs.open(path, 'r') as f:\n",
    "    zmetadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656aabd-82a6-4be2-8f08-d7543a12ed8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zmetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed061a08-6536-4dbd-ad6e-e9f512076cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zmetadata[\"metadata\"][\".zattrs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163f6549-cda4-42de-9d86-b1ccaf0e448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zmetadata[\"metadata\"][\"2m_temperature/.zarray\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c486c878-c869-4aa4-9168-3102cf31cee6",
   "metadata": {},
   "source": [
    "## 3. Setting Up the Distributed Computing Environment\n",
    "\n",
    "First, we'll set up a distributed computing cluster using Dask and SLURM to handle the large dataset efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b8d6f-0a5d-42c1-bceb-4d2aed69b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b27d5-ee14-4c21-a472-d4f71e2524f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(\n",
    "    # name=#...,\n",
    "    cores=4,\n",
    "    memory=\"8GB\",\n",
    "    processes=True,\n",
    "    scheduler_options={\"dashboard_address\": \":0\"}\n",
    ")\n",
    "client = Client(cluster)  # Connect to distributed cluster and override default\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2281ead-585a-4e47-839d-c6dbb4918245",
   "metadata": {},
   "source": [
    "Deploy 2 workers for your Dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08eb96d-c6e0-401b-a87e-b60a54f89a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d1ea9-7d0b-43ba-8fd5-5cc63bb4e1df",
   "metadata": {},
   "source": [
    "### Now open the ERA5 dataset using `xarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf592b40-7f9f-48e3-ad61-5f9e8d7a33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = xr.open_zarr(\n",
    "    'gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3',\n",
    "    # chunks=None,\n",
    "    chunks={},\n",
    "    consolidated=True,\n",
    "    storage_options=dict(token='anon'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92feb723-e3bc-4bdf-ab57-7fc15ae3424f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469cdde7-5147-4188-91a7-bff5339a07f2",
   "metadata": {},
   "source": [
    "Our dataset has `time`, `latitude`, and `longitude` for dimensions. This makes our lives easier. For more info, check this Python [short-course](https://projectpythia.org/AtmosCol-2023/) in Spanish!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5503ea4-d837-480c-8aaf-7a0fdc416198",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bytes = ds.nbytes\n",
    "print(f\"Dataset size: {total_bytes / 1024**5:.2f} PiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10212055-532d-4ca8-a058-9e47a4cc4fae",
   "metadata": {},
   "source": [
    "## 4. Data Slicing & Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303d4b2-3c08-455a-89c9-a91817160a8b",
   "metadata": {},
   "source": [
    "### 4.1 Data selection\n",
    "Understanding ERA5's arrays (Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00fbf31-6a0d-41f7-99c0-716715dfc759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds.data_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60ce2b-f395-40d0-88b6-7bea85664177",
   "metadata": {},
   "source": [
    "Let's get the 2 meter above the ground temperature `2m_temperature` array - variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5e87a-dbb2-4ef0-8001-f131a54b3e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m =  ds['2m_temperature']  # or ds.t2m\n",
    "t2m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487dc3e-a6c1-4289-a5d3-d9d7eeb267b5",
   "metadata": {},
   "source": [
    "Similarly, we can check the size of the temperature array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb7d7db-6fdc-4258-8dd9-fea762706a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Temperature array size: {t2m.nbytes / 1024**4:.2f} TB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a770346-c287-4897-9cc7-91e835401c7e",
   "metadata": {},
   "source": [
    "As we can se the chunksize, size and other aspects that we can get out of the `metadata` file without putting any data in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f379b-f517-4452-be52-542ee075e6bf",
   "metadata": {},
   "source": [
    "> ð¡ **An important aspect to consider is the way data is chunked. It will depend on how data will be queried.** Check this [link](https://blog.lobelia.earth/arco-the-smartest-way-to-access-big-geospatial-data-eaf689eff3c9) for further information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e351e-ed2b-4a69-bc52-9f5edfb4ce39",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e6f0ff; border-left:5px solid #3399ff; padding:10px; margin:10px 0; height:300px; overflow-y:auto;\">\n",
    "  <p align=\"center\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*TpY57PSaWMX__S4_fXkXAQ.png\" alt=\"Cloud Zarr\" style=\"width:45%;\">\n",
    "    <br>\n",
    "    <em style=\"color:#3366cc;\">Figure: Cloud-native architecture of Zarr enabling scalable storage and retrieval.</em>\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284f956-0091-4101-bf95-a4420a6eaed4",
   "metadata": {},
   "source": [
    "### 4.2. Data slicing\n",
    "\n",
    "#### 4.2.1. Time slicing\n",
    "Understanding ERA5's time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa67277-3efa-4330-ae2c-e134f7946796",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Time range: {ds.time.min().values} to {ds.time.max().values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a84a8b-5ea3-4302-b027-f7ee9e09301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time dimension analysis\n",
    "time_res_ns = int(ds.time.diff('time')[0].values)  # nanoseconds\n",
    "time_res_hours = time_res_ns / (1e9 * 3600)        # convert to hours\n",
    "\n",
    "# Calculate total time span\n",
    "total_ns = int(ds.time[-1].values - ds.time[0].values)\n",
    "total_days = total_ns / (1e9 * 3600 * 24)\n",
    "total_years = total_days / 365.25\n",
    "\n",
    "# Count of time steps\n",
    "n_steps = len(ds.time)\n",
    "\n",
    "print(f\"{total_days:,.0f} days\")\n",
    "print(f\"{total_years:,.0f} years\")\n",
    "print(f\"\\nNumber of time steps: {n_steps:,}\")\n",
    "print(f\"Expected steps: {int(total_days * 24):,} (hourly data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c433614-4a7d-4acf-b563-0e0ff9d8f251",
   "metadata": {},
   "source": [
    "* **Historical Reanalysis (1900-1978)**\n",
    "     - Lower quality than main ERA5\n",
    "     - Uses fewer observations\n",
    "     - Primarily useful for long-term climate studies\n",
    "     - Note: The official ERA5 starts in 1950, but this extended version includes preliminary data back to 1900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff3a6e-e817-4b6e-93da-d6ceca8d223e",
   "metadata": {},
   "source": [
    "We can use [`xarray.Dataset.sel`](https://docs.xarray.dev/en/v2023.10.1/generated/xarray.Dataset.sel.html) or [`xarray.DataArray.sel`](https://docs.xarray.dev/en/latest/generated/xarray.DataArray.sel.html) or  method to slice data along a given dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d59050-11c1-46a5-9140-1311113866a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "temp_hist = t2m.sel(time=slice('1900', '1978'))\n",
    "temp_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48fcba-dad4-446d-9f90-6b430ec28547",
   "metadata": {},
   "source": [
    "we can now see we have less number of timestamps along the `time` dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76565e-5455-4a9e-8e82-665363f0d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Historical Temperature size: {temp_hist.nbytes / 1024**4:.2f} TB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2671bdc8-664e-4722-ad6c-351f3e88e5f1",
   "metadata": {},
   "source": [
    "* **Main ERA5 Reanalysis (1979-Present)**\n",
    "     - Highest quality period\n",
    "     - Incorporates satellite era data\n",
    "     - Hourly temporal resolution\n",
    "     - 0.25Â° spatial resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25bac72-01bb-497e-b8d2-98d8d63599fb",
   "metadata": {},
   "source": [
    "*  **ERA5 Forecasts (Future Dates)**\n",
    "  \n",
    "        - Model projections beyond present day\n",
    "        - Multiple ensemble members possible\n",
    "        - Useful for testing model systems\n",
    "        - Not a true forecast - generated with fixed sea surface temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc6895-7929-4440-90ce-520db8e93ee4",
   "metadata": {},
   "source": [
    "#### 4.2.2. Lat - Lon Slicing\n",
    "We can select data using `longitude`, `latitude`, and `time` coordinates directly. This is a game changer in geospatial data analysis!\n",
    "\n",
    "Instead of dealing with numerical indexes, we can now use the intuitive .sel() method to slice data by real-world coordinates.\n",
    "\n",
    "Letâs take a look at an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc00d347-b3d8-4394-8079-06fd14e76be0",
   "metadata": {},
   "source": [
    "In this dataset, the `longitude` values range from `0Â°` to `360Â°`, and `latitude` ranges from `80Â°` to `-80Â°` (i.e., from North to South).\n",
    "\n",
    "To extract data over Colombia, we can use the .sel() method with coordinate slices like this:\n",
    "\n",
    "```python \n",
    ".sel(longitude=slice(275, 300), latitude=slice(14, -3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d9bfe8-c18d-4a6a-a13a-dfe02e3ccfe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2m_col = t2m.sel(longitude=slice(275, 300), latitude=slice(14, -3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cf0cf-46fe-4705-98d3-dfbf9e4f5800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t2m_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b0611d-3fbd-450f-9b07-a8a08ec914aa",
   "metadata": {},
   "source": [
    "In this dataset, the spatial resolution has been significantly reduced.\n",
    "\n",
    "Instead of the traditional **1440 longitude points** and **720 latitude points** (common in full-resolution ERA5), we now have:\n",
    "\n",
    "- longitude: **101 points**\n",
    "- latitude: **69 points**\n",
    "\n",
    "This coarser grid reduces data size and speeds up processing â perfect for learning, prototyping, or regional analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d5dc2-3dc5-4f9b-915f-087c125767d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Historical Temperature size: {t2m_col.nbytes / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc660cea-bd4f-4a48-a552-c9c281087948",
   "metadata": {},
   "source": [
    "## 5. Temperature Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04126f-9f40-4f71-ba37-398e9952e8a0",
   "metadata": {},
   "source": [
    "**Concept: Lazy Loading + Data Streaming**\n",
    "\n",
    "In this section, we demonstrate how to visualize massive ERA5 temperature datasets without loading the entire dataset into memory.\n",
    "\n",
    "Thanks to `Dask` and `Zarr`, data is accessed using lazy loading â meaning only the required chunks are **fetched and streamed on-demand**, right when needed. This makes it possible to work efficiently with terabyte-scale datasets using standard hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2494977b-f4e0-45e0-bfea-f134e9d3f747",
   "metadata": {},
   "source": [
    "â This approach is ideal for scalable, interactive analysis of climate and weather data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa631c-369d-4e08-8fcf-10f11c37ba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef53952-30c0-4426-a8ca-5be033241ff0",
   "metadata": {},
   "source": [
    "### 5.1. Basic Streaming Plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5617e55-ec27-4ca8-80f6-e7542aa436ad",
   "metadata": {},
   "source": [
    "This plot displays the 2-meter air temperature (t2m) over the entire spatial domain at a single time step:\n",
    "January 1, 2015 at 00:00 UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260bb4d-def2-43d0-8824-e069c79a1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "t2m.sel(time=\"2015-01-01 00:00\").plot()\n",
    "ax.set_title(\"2015-01-01 00:00 temperature\" )\n",
    "plt.savefig(\"img/era-temp2.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fee0ec-f69a-4e9d-99a0-a86ee5cb1977",
   "metadata": {},
   "source": [
    "So far, we've only queried a **single time slice** of temperature data â just one chunk. But with `Dask` and `Zarr`, we can go further: run **scalable, parallel operations** across time or space without loading everything into memory.\n",
    "\n",
    "ð This enables large-scale analysis on modest hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38cdb44-0de1-428e-aed0-71607179e929",
   "metadata": {},
   "source": [
    "### 5.2. Mean Temperature Trend for Colombia â January 2010\n",
    "Letâs compute and visualize the mean **2-meter temperature** over **Colombia** during **January 1st, 2010**. This will give us an idea of the spatial temperature distribution across the country on that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b3adb-4dbe-4801-a4ab-5ee3ad0f0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_temp_2010 = t2m_col.sel(time=\"2010-01-01\") - 273.15 # Convert to degrees Celcius\n",
    "col_temp_2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c18604-65a3-48b8-be42-3309c937efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Colombian temperature for 2010-01-01: {col_temp_2010.nbytes / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504146e-795c-4cca-addf-f95cff04d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_temp_2010 = col_temp_2010.mean(\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739520f7-2dca-47d3-93d1-3719a8baf95a",
   "metadata": {},
   "source": [
    "Up to this point, we've only built up lazy operations â no actual computation or data loading has happened yet. Now, letâs finally trigger computation and bring some data into memory!\n",
    "\n",
    "ð§  This is where Dask turns your delayed operations into real results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a576467-2981-4c84-bc37-60240a02bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_temp_2010.plot(\n",
    "    cmap=\"coolwarm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff805db3-9491-483b-ac50-ba7b19776b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_temp_2010.plot.contourf(\n",
    "    cmap=\"coolwarm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be68e7-8c23-4636-92d6-222fbca5bfd5",
   "metadata": {},
   "source": [
    "Other commands that will fetch/download data are \n",
    "\n",
    "- `.values`\n",
    "- `.data`\n",
    "- `.to_numpy()`\n",
    "- `.to_dataframe()`, `.to_series()`\n",
    "- `.compute()`\n",
    "\n",
    "or when saving data \n",
    "\n",
    "- `.to_netcdf()`\n",
    "- `.to_zarr()`\n",
    "- `.to_csv()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69394e16-07d1-422c-b64c-862aaeb46e56",
   "metadata": {},
   "source": [
    "let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18655142-2f06-4b62-bfd7-7daa4866d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_temp_2010.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c69882-4e56-4cce-9ff8-0970f8ab6b8e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#ffdddd; border-left:6px solid #f44336; padding:16px; font-family:Arial, sans-serif; font-size:16px;\">\n",
    "\n",
    "<h2 style=\"color:#b30000;\">â ï¸ WARNING: Do NOT operate directly on the full <code>ds</code> ERA5 dataset!</h2>\n",
    "\n",
    "<p>This dataset represents <strong>~3.27 Petabytes (PB)</strong> of data.</p>\n",
    "\n",
    "<p>Any direct operation on <code>ds</code> (such as <code>.plot()</code>, <code>.values</code>, <code>.to_numpy()</code>, <code>.mean()</code>, etc. without subsetting) will <strong>trigger massive data fetching</strong> and <strong>very likely crash your session or overload the backend</strong>.</p>\n",
    "\n",
    "<h3>â Best practices:</h3>\n",
    "<ul>\n",
    "  <li>Use <code>.isel()</code> or <code>.sel()</code> to select a small chunk of data before any processing.</li>\n",
    "  <li>Use <code>.compute()</code> or <code>.load()</code> <strong>only</strong> after subsetting.</li>\n",
    "  <li>Avoid calling <code>.plot()</code> or accessing <code>.values</code> on the full dataset.</li>\n",
    "</ul>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c767f3c1-e0fc-42b7-8ec3-07f04e551422",
   "metadata": {},
   "source": [
    "## 6. Release the resources reserved on the High-Performance Computing System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b822c5-2cdf-4d93-bca0-151ebb3fd3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad174c9-63d8-43c3-9757-2cee6318620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c59c2f-78ae-454e-aee9-ec947f1d0d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
